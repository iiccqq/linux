



Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:

* Oracle JDK 1.7 (preferred)
  $ sudo apt-get purge openjdk*
  $ sudo apt-get install software-properties-common
  $ sudo add-apt-repository ppa:webupd8team/java
  $ sudo apt-get update
  $ sudo apt-get install oracle-java8-installer
  //原文档是写的7，不过目前没有7了，只有8.sudo apt-get install oracle-java7-installer
* Maven
  $ sudo apt-get -y install maven
* Native libraries
  $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev
* ProtocolBuffer 2.5.0 (required)
  $ sudo apt-get -y install libprotobuf-dev protobuf-compiler


Optional packages:

* Snappy compression
  $ sudo apt-get install snappy libsnappy-dev
* Bzip2
  $ sudo apt-get install bzip2 libbz2-dev
* Jansson (C Library for JSON)
  $ sudo apt-get install libjansson-dev
* Linux FUSE
  $ sudo apt-get install fuse libfuse-dev



When you import the project to eclipse, install hadoop-maven-plugins at first.
  $ cd hadoop-maven-plugins
  $ mvn install

Then, generate eclipse project files.
  $ mvn eclipse:eclipse -DskipTests


At last, import to eclipse by specifying the root directory of the project via
[File] > [Import] > [Existing Projects into Workspace].


Building distributions:

Create binary distribution without native code and without documentation:

  $ mvn package -Pdist -DskipTests -Dtar

Create binary distribution with native code and with documentation:

  $ mvn package -Pdist,native,docs -DskipTests -Dtar

Create source distribution:

  $ mvn package -Psrc -DskipTests

Create source and binary distributions with native code and documentation:

  $ mvn package -Pdist,native,docs,src -DskipTests -Dtar

Create a local staging version of the website (in /tmp/hadoop-site)

  $ mvn clean site; mvn site:stage -DstagingDirectory=/tmp/hadoop-site




Pseudo-Distributed Operation

etc/hadoop/core-site.xml:

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

etc/hadoop/hdfs-site.xml:


<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>


  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ export HADOOP\_PREFIX=/home/vagrant/opt/hadoop-2.7.1



  $ ssh localhost
/etc/ssh/ssh_config: line 55: Bad configuration option: usedns
/etc/ssh/ssh_config: terminating, 1 bad configuration options


注释掉 UseDNS no行。

配置hadoop环境变量PATH
sudo vi /etc/profile
export PATH=$PATH:/home/vagrant/opt/hadoop-2.7.1/bin


hdfs namenode -format
start-dfs.sh

  $ hdfs dfs -mkdir /user
  $ hdfs dfs -mkdir /user/vagrant

  $ hdfs dfs -put etc/hadoop input
  $ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'

  $ hdfs dfs -get output output
  $ cat output/*
  或者
  hdfs dfs -cat output/*

配置hadoop环境变量PATH
sudo vi /etc/profile
export PATH=$PATH:/home/vagrant/opt/hadoop-2.7.1/sbin

export PATH=$PATH:/home/vagrant/opt/hadoop-2.7.1/sbin

start-yarn.sh