

官方文档：
http://hadoop.apache.org/docs/r2.9.0/hadoop-project-dist/hadoop-common/ClusterSetup.html


下载hadoop
wget http://apache.website-solution.net/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
sudo yum install -y rsync

修改hosts
sudo vi /etc/hosts
192.168.56.101 NameNode
192.168.56.102 ResourceManager
192.168.56.103 Slave1
192.168.56.104 Slave2


环境变量
sudo vi  etc/hadoop/hadoop-env.sh 
export JAVA_HOME=/opt/jdk1.8.0_144


sudo vi /etc/profile.d/hadoop.sh

export HADOOP_HOME=/opt/hadoop-2.9.0
HADOOP_PREFIX=/opt/hadoop-2.9.0
export HADOOP_PREFIX
export HADOOP_CONF_DIR=/opt/hadoop-2.9.0/etc/hadoop
export HADOOP_YARN_HOME=/opt/hadoop-2.9.0
export PATH=$PATH:$HADOOP_PREFIX/bin

source /etc/profile

vi etc/hadoop/core-site.xml

 <property>
        <name>fs.defaultFS</name>
        <value>hdfs://NameNode:9000</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    

vi etc/hadoop/hdfs-site.xml


Configurations for NameNode:
Configurations for DataNode:

   
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/vagrant/namenodedir</value>
    </property>	
    
	 <property>
        <name>dfs.blocksize</name>
        <value>268435456</value>
    </property>
	 <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
    </property>	
	
	 <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/vagrant/datanodedir</value>
    </property>
	
	 mkdir -p /home/vagrant/namenodedir 
	 mkdir -p /home/vagrant/datanodedir 

Configurations for ResourceManager and NodeManager:
vi etc/hadoop/yarn-site.xml

    <property>
        <name>yarn.acl.enable</name>
        <value>false</value>
    </property>
	  <property>
        <name>yarn.admin.acl</name>
        <value>*</value>
    </property>
	  <property>
        <name>yarn.log-aggregation-enable</name>
        <value>false</value>
    </property>	
	  <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>ResourceManager</value>
    </property>
如果提示。。虚拟机内存小。可以增加以下配置
	<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>

	
  在NameNode和ResourceManager上执行
  $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys
  $	ssh-copy-id -i ~/.ssh/id_rsa.pub NameNode
  $	ssh-copy-id -i ~/.ssh/id_rsa.pub ResourceManager
  $	ssh-copy-id -i ~/.ssh/id_rsa.pub Slave1
  $	ssh-copy-id -i ~/.ssh/id_rsa.pub Slave2  
  
  在NameNode机器上运行：
	
	 [hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format fc	 
	 [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
	 [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
	 [hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh
	 
  在ResourceManager机器上运行：
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver
	[yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh
	[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver
	
	
	[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
	[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
	[hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh
	
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR stop nodemanager
	[yarn]$ $HADOOP_PREFIX/sbin/stop-yarn.sh
	[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver
	[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver
	
	
	查看日志位置：$HADOOP_PREFIX/logs下
	hdfs dfs -rm -skipTrash /tmp/*
	hdfs dfs -rmr /tmp/*
	hdfs dfs -expunge
